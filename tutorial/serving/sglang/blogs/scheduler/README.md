[Batching](https://www.notion.so/Batching-2008a0af5e7f80a4803aeb796320b234?pvs=21)

## Overview

ScheduleBatchï¼šStore all information of a batch on the scheduler.

maintain the following important states

```python
running_batch: ScheduleBatch(reqs=[]) # running_batch
last_batch: Optional[ScheduleBatch] # last forward batch
cur_batch: Optional[ScheduleBatch] # current forward batch
waiting_queue: List[Req] # waiting queue
```

### Log

`Scheduler Args` :

- `forward_ct=0` : used in the `watchdog`
- `forward_ct_decode=0` : control the log frequency
  - `self.forward_ct_decode *%* self.server_args.decode_log_interval *==* 0`
- `num_generated_tokens=0` : added in the `process_batch_result_decode`
- `num_prefill_tokens=0` : unused
- `last_decode_stats_tic=time.perf_counter()`
- `last_prefill_stats_tic=time.perf_counter()` : unused

```python
def run_batch():
  forward_ct += 1
```

**`Log Prefill:`**

1. num_new_seq: `len(can_run_list)`
2. new-token: `adder.log_input_tokens`
3. cached-token: `adder.log_hit_tokens`
4. token usage
   1. `num_used:`
      1. `unused: token_to_kv_pool_allocator.available_size() + tree_cache.evictable_size()`
      2. `max_total_num_tokens - unused_num_tokens`
   2. `num_used / max_total_num_tokens`
5. #queue-req: `len(self.waiting_queue)`

**`Log decode:`**

1. #running-req: `num_running_reqs`
2. #token: `num_used`
3. token usage: same to above
4. cuda graph: `can_run_cuda_graph`
5. gen throughput (token/s): `self.num_generated_tokens */* gap_latency`
6. #queue-req: `len(self.waiting_queue)`

### Watch Dog

**`Server Args`**:

- `watchdog_timeout`: float ("Set watchdog timeout in seconds. If a forward batch takes longer than this, the server will crash to prevent hanging.")

```python
def watchdog_thread():
  self.watchdog_last_forward_ct = 0
  self.watchdog_last_time = time.perf_counter()

  while True:
    current = time.perf_counter()
    if self.cur_batch is not None:
      if self.watchdog_last_forward_ct == self.forward_ct:
        # timeout check
        if current > self.watchdog_last_time + self.watchdog_timeout:
          break
      else:
        self.watchdog_last_forward_ct = self.forward_ct
        self.watchdog_last_time = current
    time.sleep(self.watchdog_timeout // 2)
  # dump the process

  # kill the parent_process: `TokenizerManager`
  self.parent_process.send_signal(signal.SIGQUIT)
```

## Scheduler Event Loop

```python
def event_loop_overlap():
  self.result_queue = queue()
  while True:
    recv_reqs = self.recv_requests()
    self.process_input_requests(recv_reqs)

    batch = self.get_next_batch_to_run()
    self.cur_batch = batch


    result = self.run_batch(batch)
    self.process_batch_result(result)

    self.last_batch = batch
```

1. `recv_requests` : Receive results at `tp_rank = 0` and broadcast it to all other TP ranks.

```python
def recv_requests(self) -> List[Req]:
    """Receive results at tp_rank = 0 and broadcast it to all other TP ranks."""
    if self.attn_tp_rank == 0:
        recv_reqs = []
         while True:
            try:
                recv_req = self.recv_from_tokenizer.recv_pyobj(zmq.NOBLOCK)
            except:
                ...
   else:
     recv_reqs = []

    return recv_reqs
```

1. `process_input_requests` ï¼šå°†`TokenizedGenerateReqInput` ç­‰å°è£…æˆ`Req` ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­è¿›è¡Œä¸€äº›å‚æ•°æ£€æŸ¥å’Œè®°å½•

   1. `handle_generate_request` : å°†å„ç§ Input è½¬æ¢ä¸º`Req`
   2. `_add_request_to_queue` : æ ¹æ®å½“å‰ Scheduler çš„ç±»å‹å°†`Req` åŠ å…¥åˆ°ä¸åŒçš„é˜Ÿåˆ—ä¸­ï¼Œå…¶ä¸­å¦‚æœæ˜¯æ··åˆï¼ˆé»˜è®¤ï¼‰Scheduler çš„è¯ï¼Œå°±æ˜¯å°†`Req`åŠ å…¥åˆ°`waiting_queue` ä¸­

1. `get_next_batch_to_run` ï¼šè·å¾—ä¸€ä¸ª`ScheduleBatch` å¯¹è±¡ (**CORE**)

> Why `prefill` is prioritized?
> TTFT!

```python
def get_new_batch_to_run():
  # update running_batch
  if last_batch.forward_mode == "extend":
    # consider chunk prefill
    ...

    # add prefilled batch into running_batch for next schedule for decode
    running_batch.merge(last_batch)

  # first consider prefilling
  new_batch = get_new_batch_prefill()
  if new_batch is not None:
    return new_batch # let the cur_batch be this prefill batch

  # run decode by filter some finished batch
  running_batch = update_running_batch(running_batch)
  return running_batch
```

`get_new_batch_prefill`:

```python
def get_new_batch_prefill():
  if (running_batch.is_full or len(waitting_queue) == 0) and chunk_req is None:
    return

  running_bs = len(running_batch.reqs)

  # whether the prefix has been computed
  # sort the reqs in the waiting queue by policies
  prefix_computed = policy.calc_priority(waiting_queue)

  addr = PrefillAdder(tree_cache, ...)

  for req in waiting_queue:
    # get prefix_indices & last_node from tree_cache => compute extend_input_len
    # put above info into req obj which is used for whether add it into new_batch
    req.init_next_round_input()

    # check whether the extended token num surpass accountable tokens
    # if not surpass, add this req into addr.can_run_list
    addr.add_one_req(req, ...)


  waiting_queue.remove(addr.can_run_list)

  # create a new `ScheduleBatch`
  new_batch = ScheduleBatch.init_new(can_run_list)

  # allocate resources
  new_batch.prepare_for_extend()

  return new_batch
```

<aside>
ğŸ’¡

Scheduler Policy mainly controls the `get_new_prefill_batch`

`Server Args: schedule_policy=â€fcfsâ€`

```python
# scheduler init process
self.policy = SchedulePolicy(schedule_policy, tree_cache, enable_hi_cache)
```

`Server Args`

```python
class PrefillAdder:
  def __init__(self):

  def add_one_req(req, ...):
    # estimate the total_tokens (to eos_token) used by this req
    total_tokens = req.extend_input_len + min(
            req.sampling_params.max_new_tokens, CLIP_MAX_NEW_TOKENS_ESTIMATION
        )

        # get this num_input_tokens (n * page_size)
        input_tokens = (
            -(-req.extend_input_len // self.tree_cache.page_size)
            * self.tree_cache.page_size
        )

        # get prefix_len tokens in this req
        prefix_len = len(req.prefix_indices)

        if total_tokens >= self.rem_total_tokens:
          # meaning the required total tokens by this req cannot be satified
          return AddReqResult.NO_TOKEN

      if input_tokens > self.rem_input_tokens and len(self.can_run_list) != 0:
            # extend_len tokens > rem_input_tokens and
            return AddReqResult.OTHER

    with self._lock_node(req.last_node):

```

There are totally Policies:

1. `CacheAware` (require prefix matches)
   1. `Longest Prefix`
   2. `DFS-Weight`

```python
# in SechdulePolicy
def _compute_prefix_matches():
  waiting_queue_radix_tree.reset()

  for req in waiting_queue:
    # get req's prefix ids
    prefix_ids = req.adjust_max_prefix_ids()

    # use tree cache to match (rid is not used in matching prefix)
    req.prefix_indices, req.last_node = tree_cache.match_prefix(req.rid, prefix_ids)

```

1. `CacheAgnostic`
   1. `FCFS`
   2. `Longest output first`
   3. `Random`

</aside>

`update_running_batch`:

```python
def update_running_batch(running_batch):
  initial_bs = running_batch.batch_size()

  running_batch.filter_batch()

  # check if decode out of memory

  if running_batch.batch_size() < initial_bs:
    running_batch.batch_is_full = False

  running_batch.prepare_for_decode()
  return running_batch
```

1. `run_batch` ï¼šçœŸæ­£è¿è¡Œ forward çš„åœ°æ–¹ï¼Œè¿”å›ä¸€ä¸ª`GenerationBatchResult` (ä¸´æ—¶å°è£…äº§ç”Ÿä¸€ä¸ª token çš„ç»“æœ) ä»`ScheduleBatch`ä¸­è·å¾—`ModelWorkerBatch` ï¼Œ(`ScheduleBatch`çš„å†é«˜ä¸€å±‚çš„å°è£…ï¼‰

```python
def run_batch():

 # get model_worker_batch from `ScheduleBatch`
 model_worker_batch = batch.get_model_worker_batch()

 # tp worker to forward
  logits_output, next_token_ids, ... \
      = tp_worker.forward_batch_generation(model_worker_batch)

 batch.output_ids = next_token_ids
 bid = model_worker_batch.bid

 # wrap results
 ret = GenerationBatchResult(logits_output, ... , next_token_ids, bid, ...)
 return ret
```

1. `process_batch_result`

å¤„ç†çš„è¿‡ç¨‹ä¸­ï¼Œæ ¹æ®`forward_mode`æ¥é€‰æ‹©`process`çš„æ–¹æ³•ï¼Œå°†`ScheduleBatch` ä¸`GenerationBatchResult` ä½œä¸ºå‚æ•°è¿›è¡Œå¤„ç†å¯¹æ¯”

ç»™`send_to_detokenizer` å‘é€ `BatchTokenIDOut`

```python
def process_batch_result(
    self,
    batch: ScheduleBatch,
    result: Union[GenerationBatchResult, EmbeddingBatchResult],
    launch_done: Optional[threading.Event] = None,
):
    if batch.forward_mode.is_decode():
        self.process_batch_result_decode(batch, result, launch_done)
    elif batch.forward_mode.is_extend():
        self.process_batch_result_prefill(batch, result, launch_done)
    elif batch.forward_mode.is_idle():
        if self.enable_overlap:
            self.tp_worker.resolve_last_batch_result(launch_done)
            if batch.next_batch_sampling_info:
                batch.next_batch_sampling_info.update_regex_vocab_mask()
                self.current_stream.synchronize()
                batch.next_batch_sampling_info.sampling_info_done.set()
    elif batch.forward_mode.is_dummy_first():
        batch.next_batch_sampling_info.update_regex_vocab_mask()
        self.current_stream.synchronize()
        batch.next_batch_sampling_info.sampling_info_done.set()
```

## Overlap

<https://github.com/sgl-project/sglang/blob/85e1a6f3aa5a2288ca85fe3fe922c733b6533fa7/python/sglang/srt/managers/scheduler.py#L399>

initialize a `deque`

<https://github.com/sgl-project/sglang/pull/1677/>

<https://github.com/sgl-project/sglang/pull/1687/>

## TP Worker

æ¯ä¸ª Scheduler éƒ½æœ‰ä¸€ä¸ª Worker

`Reqs` â‡’ `ScheduleBatch` â‡’ `ModelWorkerBatch` â‡’ `ForwardBatch`

åˆå§‹åŒ–

- `ModelConfig`
- `ModelRunner` : å°†`ModelConfig` ä¼ å…¥

ç›®çš„ï¼šæä¾›ä¸€å±‚æŠ½è±¡ï¼šå°†`ModelWorkerBatch` è½¬åŒ–ä¸º`ForwardBatch` çš„

ä¸»è¦å‡½æ•°ï¼š

`forward_batch_generation`

1. ä»`ModelWorkerBatch`è·å¾—`ForwardBatch` : è·å¾—ä¸€æ¬¡ Forward çš„æ‰€æœ‰ä¿¡æ¯
   - åŒ…å«`AttentionBackend`

- ä½¿ç”¨`ModelRunner` `forward` è¿™ä¸ª`forward_batch`

- è°ƒç”¨`ModelRunner` `sample`

## ModelRunner

çœŸæ­£æ¨¡å‹å‰å‘è®¡ç®—çš„åœ°æ–¹

åœ¨`init`è¿‡ç¨‹ä¸­ï¼Œä¼šå¯åŠ¨

1. model
2. init memory

model forward çš„ç»Ÿä¸€æ¥å£

### Model

æ¨¡å‹çš„åˆ›å»ºä¸åŠ è½½ï¼ŒSGLang æ‰€æ”¯æŒçš„æ¨¡å‹éƒ½ä½äº<https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/models>

å…¶ä¸­ä¸€äº›æ¨¡å‹å¸¸ç”¨åˆ°çš„`layer` éƒ½æ˜¯è‡ªå®šä¹‰å†™å¥½çš„ï¼Œä½äº<https://github.com/sgl-project/sglang/tree/main/python/sglang/srt/layers>

è¿™äº› model ç±»é™¤äº†æ­£å¸¸çš„ init, forward å®šä¹‰å¤–ï¼Œéƒ½ç»Ÿä¸€å®šä¹‰äº†`load_weights` æ¥åŠ è½½æƒé‡

æœ€åå®šä¹‰`EntryClass` æ¥è¡¨ç¤ºå…¥å£çš„ç±»

ç”¨æ¥å»ºç«‹ä¸€ä¸ª`str â‡’ Class`çš„æ˜ å°„ï¼Œä¸ºäº†æ¨¡å‹æ¶æ„çš„åŠ è½½

æ ¸å¿ƒçš„`radix attention`:

åŸºäºä¸åŒçš„`attn_backend`æ¥åšå¯¹åº”çš„`forward`

```python
    def forward(
        self,
        q,
        k,
        v,
        forward_batch: ForwardBatch,
        save_kv_cache: bool = True,
        **kwargs,
    ):
        if k is not None:
            # For cross-layer sharing, kv can be None
            assert v is not None
            if "k_rope" not in kwargs:
                k = k.view(-1, self.tp_k_head_num, self.qk_head_dim)
                v = v.view(-1, self.tp_v_head_num, self.v_head_dim)
            else:
                k = k.view(-1, self.tp_k_head_num, self.v_head_dim)

        return forward_batch.attn_backend.forward(
            q,
            k,
            v,
            self,
            forward_batch,
            save_kv_cache,
            **kwargs,
        )
```

# Load Balance

1. ROUND_ROBIN

```python
  self.round_robin_counter = 0

  def round_robin_scheduler(self, req):
      self.workers[self.round_robin_counter].send_pyobj(req)
      self.round_robin_counter = (self.round_robin_counter + 1) % len(self.workers)
```

2. SHORTEST_QUEUE
