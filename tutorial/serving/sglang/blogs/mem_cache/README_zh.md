# å†…å­˜åˆ†é…ä¸ç¼“å­˜ç®¡ç†

[Orginal Version(English)](./README.md)

æ‰§è¡Œæµç¨‹ï¼š
`launch_server` â‡’ `_launch_subprocesses` â‡’ `Init Scheduler` â‡’ `Init TpWorker` â‡’ `Init ModelConfig & ModelRunner` â‡’ `ModelRunner init KV Cache Pool & Allcator`

ä¸»è¦åŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š

1. `KV Cache`åˆå§‹åŒ–ä¸­`mem-fraction-static`çš„å·¥ä½œåŸç†
2. æ¯ä¸ªtokençš„KVç¼“å­˜å¦‚ä½•è®¡ç®—
3. KVç¼“å­˜æ± çš„ç®¡ç†æœºåˆ¶ï¼ˆåˆ†é…ã€é‡Šæ”¾ã€ä½¿ç”¨ï¼‰
4. Radix Treeæ˜¯å¦‚ä½•ç®¡ç†å’Œå¤ç”¨`KV Cache`

æœ‰ä»¥ä¸‹ä¸¤ä¸ªç« èŠ‚
â€‹â€‹- `KV Cache`ç®¡ç†â€‹â€‹ï¼šæ¢è®¨å¦‚ä½•é€šè¿‡åˆ†é…ã€é‡Šæ”¾å’Œä½¿ç”¨æ¥ç®¡ç†`KV Cache`
â€‹- â€‹`Radix Tree Cache`â€‹â€‹ï¼šæ¢è®¨åŸºæ•°æ ‘æ•°æ®ç»“æ„å¦‚ä½•å®ç°KVç¼“å­˜å¤ç”¨

## `KV Cache`ç®¡ç†
>
> â€‹â€‹èƒŒæ™¯çŸ¥è¯†â€‹â€‹
ModelRunnerï¼šæŒæœ‰å®é™…æ¨¡å‹ï¼Œè´Ÿè´£æ‰§è¡Œæ¨¡å‹çš„â€‹â€‹å‰å‘ä¼ æ’­â€‹

ä»¥ä¸‹æ˜¯ModelRunnerçš„åˆå§‹åŒ–è¿‡ç¨‹ï¼ŒåŒæ—¶ä¹Ÿæ˜¯KVç¼“å­˜æ± çš„åˆå§‹åŒ–è¿‡ç¨‹

åœ¨åˆå§‹åŒ–å†…å­˜æ± æ—¶ï¼ŒSGLangæä¾›äº†ä¸‰ä¸ªæŠ½è±¡ç®¡ç†å™¨ï¼š

req_to_token_poolï¼šå°†è¯·æ±‚çš„tokenæ˜ å°„åˆ°out_cache_locçš„å†…å­˜æ± 
token_to_kv_poolï¼šå°†req_token_poolä¸­çš„out_cache_locæ˜ å°„åˆ°å®é™…KVç¼“å­˜æ•°æ®
token_to_kv_pool_allocatorï¼šåˆ†é…å’Œé‡Šæ”¾å®é™…KVç¼“å­˜æ•°æ®

```python
class ModelRunner:
  def __init__(self, model_config, ....):
    # è°ƒæ•´`AttentionBackend`å’Œ`mem_fraction_static`
    model_specific_adjustment()

    # ç”±äºSGLangä¼šæ ¹æ®æ¨¡å‹æ¶æ„è°ƒæ•´è®¾ç½®ï¼Œå› æ­¤éœ€è¦å…¨å±€æ›´æ–°è¿™äº›ä¿¡æ¯
    global_server_args_dict.update({...})

    # ä¸ºåç»­é€šä¿¡æ„å»ºWORLD_GROUPã€TP_GROUPã€PP_GROUP
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®¾ç½®åï¼Œè·å–å…¨å±€æœ€å°çš„GPUå†…å­˜
    min_per_gpu_memory = init_torch_distributed()

    initialize(min_per_gpu_memory)

  def initialize(min_per_gpu_memory):
    # åŠ è½½é‡‡æ ·å™¨å’Œæ¨¡å‹
    sampler = Sampler()
    load_model()

    ######
    # è‡³æ­¤ï¼Œæ¨¡å‹æƒé‡å’Œåˆ†å¸ƒå¼åˆå§‹åŒ–å·²å ç”¨éƒ¨åˆ†GPUå†…å­˜
    # æ³¨æ„ï¼šä½†`min_per_gpu_memory`ä¸ä¼šå˜åŒ–
    ######

    # æœ¬æ–‡æ ¸å¿ƒ!!!
    init_memory_pool(
      min_per_gpu_memory,
      server_args.max_running_requests,  # è¿™ä¸¤ä¸ªå‚æ•°ç”±ç”¨æˆ·è®¾ç½®
      server_args.max_total_tokens)

    # ...
    init_cublas()
    init_attention_backend()
    init_cuda_graphs()

  def init_memory_pool(
       total_gpu_memory,
       max_num_reqs=None,
       max_total_tokens=None):
    # è®¡ç®—æ¯ä¸ªGPUå¯ä»¥ä¿å­˜å¤šå°‘tokençš„KVç¼“å­˜
    max_total_num_tokens = profile_max_num_token(total_gpu_memory)

    # è°ƒæ•´max_num_requests
    if max_num_reqs is None:
      max_num_reqs = min(
       max(max_total_num_tokens / model_config.context_len * 512, 2048),
       4096
    )

    # è°ƒæ•´max_total_tokens
    if max_total_tokens is None:
      if max_total_tokens > max_total_num_tokens: logger.warning...
      max_total_num_tokens = min(max_total_tokens, max_total_num_tokens)

    # æŒ‰é¡µå¤§å°å¯¹é½
    max_total_num_tokens = (max_total_num_tokens // page_size) * page_size

    # åˆå§‹åŒ–req_to_token_pool
    req_to_token_pool = ReqToTokenPool(
           max_num_reqs + 1,
           model_config.context_len + 4,
           ...)

    # åˆå§‹åŒ–token_to_kv_pool
    token_to_kv_pool = MHATokenToKVPool(
           max_total_num_tokens,
           page_size,
           kv_cache_dtype,
           head_num,
           head_dim,
           layer_num,
           ...)

    # åˆå§‹åŒ–token_to_kv_pool_allocator
    token_to_kv_pool_allocator = TokenToKVPoolAllocator(
        max_total_num_tokens,
        kv_cache_dtype,
        device,
        token_to_kv_pool)

    ...END !!!

  def profile_max_num_token(total_gpu_memory):
    # è·å–å…¨å±€æœ€å°çš„å¯ç”¨GPUå†…å­˜
    # æ³¨æ„ï¼šæ­¤æ—¶æ¨¡å‹å·²åŠ è½½
    available_gpu_memory = get_available_gpu_memory(distributed=True)

    # è®¡ç®—å•ä¸ªtokençš„KVç¼“å­˜å ç”¨çš„GPUå†…å­˜
    # æ³¨æ„ï¼šåœ¨TPè®¾ç½®ä¸­ï¼Œæ¯ä¸ªGPUä»…å¤„ç†éƒ¨åˆ†`attention head`è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    cell_size = (
      model_config.get_num_kv_heads(get_attention_tp_size())  # è·å–TPè®¾ç½®ä¸‹çš„num_kv_headsæ•°é‡
     * model_config.head_dim
     * num_layers
     * 2  # å› ä¸ºåŒ…å«Kå’ŒV
     * element_size(kv_cache_dtype)  # KVç¼“å­˜ç±»å‹æ¯ä¸ªå…ƒç´ çš„å­—èŠ‚æ•°
    )

    # è¿™æ˜¯`mem_fraction_static`çš„æ ¸å¿ƒä½œç”¨
    # æ³¨æ„ï¼š
    # - `total_gpu_memory`æ˜¯åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒåçš„min_per_gpu_memory
    # - `available_gpu_memory`æ˜¯åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒå¹¶åŠ è½½æ¨¡å‹åçš„min_per_gpu_memory
    # - `total_gpu_memory * (1 - mem_fraction_static)`ï¼šå…¶ä»–æ½œåœ¨çš„GPUå†…å­˜ä½¿ç”¨ï¼ˆå¦‚å‰å‘ä¼ æ’­ä¸­çš„`activation`ï¼‰
    # - `rest_memory`ï¼šåŠ è½½æ¨¡å‹åçš„ç©ºé—²GPUå†…å­˜å‡å»å…¶ä»–GPUå†…å­˜ï¼Œå‰©ä½™éƒ¨åˆ†ç”¨äº`KVç¼“å­˜`
    rest_memory = available_gpu_memory - total_gpu_memory *
       (1 - mem_fraction_static)

    # å°†rest_memoryä»GBè½¬æ¢ä¸ºå­—èŠ‚å•ä½
    # è®¡ç®—å¯ä»¥ä¿å­˜å¤šå°‘tokençš„KVç¼“å­˜
    max_num_tokens = int(rest_memory * (1 << 30) // cell_size)
    return max_num_tokens
```

é€šè¿‡ä¸Šè¿°ç®€åŒ–ä»£ç ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼š

**mem_fraction_staticçš„ä½œç”¨**: mem_fraction_staticç”¨äºåˆ’åˆ†GPUå†…å­˜ç»™æ¨¡å‹æƒé‡å’ŒKVç¼“å­˜æ± ã€‚å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³é”™è¯¯ï¼Œå¯ä»¥ä½¿ç”¨æ›´å°çš„å€¼ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š

1. è·å–ç©ºé—²GPUå†…å­˜ï¼ˆM1ï¼šæ€»ç©ºé—²GPUå†…å­˜ï¼‰
2. åŠ è½½æ¨¡å‹ï¼ˆå ç”¨éƒ¨åˆ†GPUå†…å­˜ï¼‰
3. å†æ¬¡è·å–ç©ºé—²GPUå†…å­˜ï¼ˆM2ï¼šåŠ è½½æ¨¡å‹åçš„ç©ºé—²å†…å­˜ï¼‰
4. è®¡ç®—éé™æ€GPUå†…å­˜ï¼šM3 = M1 * (1 - mem_fraction_static)
5. KVç¼“å­˜æ± çš„å†…å­˜ï¼šM2 - M3

**å•ä¸ªtokençš„KVç¼“å­˜è®¡ç®—æ–¹å¼**ï¼š tp_num_head \* head_dim \* num_layers \* 2 \* element_size (torch._utils._element_size(kv_cache_dtype))

### Managers

#### req_to_token_pool

å°†è¯·æ±‚æ˜ å°„åˆ°å…¶tokenä½ç½®çš„å†…å­˜æ± ã€‚

å½¢çŠ¶ï¼šmax_num_reqs + 1 Ã— self.model_config.context_len + 4

æ•°æ®ç±»å‹ï¼štorch.int32

è®¿é—®æ–¹å¼ï¼š

- dim0ï¼šå…·ä½“çš„req_idx
- dim1ï¼šè¯·æ±‚ä¸­çš„tokenä½ç½®ï¼ˆä»0, 1, 2...å¼€å§‹ï¼‰ï¼Œæ ‡è¯†è¯·æ±‚ä¸­çš„ç‰¹å®štoken
- å€¼(out_cache_loc)ï¼šæŒ‡å‘ä¸dim0å’Œdim1æ ‡è¯†çš„tokenå…³è”çš„KVç¼“å­˜ç´¢å¼•

```python
class ReqToTokenPool:
  def __init__(size, max_context_len):
    req_to_token = torch.zeros(size, max_context_len, dtype=torch.int32)
    # è®°å½•ç©ºé—²æ§½ä½
    free_slots = list(range(size))

  def write(indices, values):
    req_to_token[indices] = values

  def avaiable_size():
    return len(free_slots)

  def alloc(need_size):
    if need_size > len(free_slots): return None
    # ç›´æ¥ç§»é™¤`need_size`ä¸ªæ§½ä½
    select_index = free_slots[:need_size]
        free_slots = free_slots[need_size:]
        return select_index

    def free(free_index):
      free_slots.extend(free_index)

  def clear():
    free_flost = list(range(size)
```

#### token_to_kv_pool

å°†req_token_poolä¸­çš„out_cache_locæ˜ å°„åˆ°å®é™…KVç¼“å­˜æ•°æ®

ä¸»è¦ç»´æŠ¤k_bufferå’Œv_bufferï¼Œä¸¤è€…å½¢çŠ¶ç›¸åŒ

å½¢çŠ¶ï¼ˆTensoråˆ—è¡¨ï¼‰ï¼šlayer_num Ã— [Tensor]ï¼Œå…¶ä¸­æ¯ä¸ªTensorï¼šmax_total_num_tokens + page_size Ã— head_num Ã— head_dim

è®¿é—®æ–¹å¼ï¼š

- dim0ï¼šlayer_idæ ‡è¯†ç‰¹å®šå±‚
- dim1ï¼šout_cache_locæ ‡è¯†ç‰¹å®šKVç¼“å­˜ç´¢å¼•
- dim2ï¼šhead
- dim3ï¼šhead_dim
- å€¼ï¼šå®é™…KVç¼“å­˜æ•°æ®

```python
class MHATokenToKVPool(KVCache):
  def __init__(size, page_size, dtype, head_num, head_dim, layer_num, device, start_layer...):
    # åˆ›å»ºå®é™…KVç¼“å­˜ç¼“å†²åŒº
    _create_buffers()
    ############
    # æ­¤æ—¶ï¼Œæ¯ä¸ªGPUå†…å­˜å‡ ä¹è€—å°½
    ###########

  def _create_buffers():
    k_buffer = [
                torch.zeros(
                    (size + page_size, head_num, head_dim),
                    kv_cache_dtype,
                    device,
                )
                for _ in range(layer_num)
            ]
        v_buffer = [
                torch.zeros(
                    (size + page_size, head_num, head_dim),
                    kv_cache_dtype,
                    device,
                )
                for _ in range(layer_num)
            ]
     def _clear_buffers():
       del k_buffer, v_buffer

   ################
   ## è¯»å–API
   ################
   def get_key_buffer(layer_id):
     return k_buffer[layer_id - start_layer]

   def get_value_buffer(layer_id):
     return v_buffer[layer_id - start_layer]

   def get_kv_buffer(layer_id):
        return get_key_buffer(layer_id), get_value_buffer(layer_id)

    ############
    ## å†™å…¥API
    ############
    def set_kv_buffer(layer, loc, cache_k, cache_v, ...):
      layer_id = layer.layer_id
      k_buffer[layer_id - start_layer][loc] = cache_k
         v_buffer[layer_id - start_layer][loc] = cache_v
```

#### token_to_kv_pool_allocator

ç”¨äºåˆ†é…å®é™…KVç¼“å­˜æ•°æ®ï¼šout_cache_loc

```python
class TokenToKVPoolAllocator:
  def __init__(size [max_total_num_tokens], dtype, page_size device, kvcache [token_to_kvcache_pool]):
    page_size = 1
    clear()

  def clear():
    free_slots = torch.arange(1, self.size + 1, dtype=torch.int64, device)

  def available_size():
    return len(free_slots)

  ##########################
  # åˆ†é…API
   #########################
  def alloc(need_size):
    if need_size > len(self.free_slots): return None
        select_index = free_slots[:need_size]
        free_slots = free_slots[need_size:]
        return select_index

    ###########################
    ## é‡Šæ”¾API
    ###########################
    def free(free_index):
     free_slots = torch.cat((free_slots, free_index))
```

**ä¸ºè¯·æ±‚å’Œout_cache_locåˆ†é…æ§½ä½**
è¿™å°±å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼šSGLangå¦‚ä½•ä½¿ç”¨ä¸Šè¿°ç®¡ç†å™¨é«˜æ•ˆåœ°ä¸ºæ¯ä¸ªè¯·æ±‚ä¸­çš„tokenåˆ†é…æ§½ä½å¹¶åŠæ—¶é‡Šæ”¾ï¼Ÿ

LLMæ¨ç†åŒ…å«ä¸¤ä¸ªä¸»è¦é˜¶æ®µã€‚æˆ‘ä»¬é¦–å…ˆç¡®å®šæ¯ä¸ªé˜¶æ®µçš„åˆ†é…éœ€æ±‚ã€‚

1. â€‹â€‹é¢„å¡«å……ï¼ˆprefillï¼‰â€‹â€‹ï¼š
    1. req_to_token_pool.allocï¼šå› ä¸ºæœ‰æ–°è¯·æ±‚
    2. token_to_kv_pool_allocator.allocï¼šå¯èƒ½ï¼Œ
        1. å¦‚æœè¯·æ±‚ä¸­çš„tokenå·²æœ‰KVç¼“å­˜ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨req_to_token_pool.writeå¤ç”¨è¿™äº›KVç¼“å­˜
        2. å¦‚æœæ²¡æœ‰KVç¼“å­˜ï¼Œåˆ™è°ƒç”¨token_to_kv_pool_allocator.allocè·å–out_cache_locï¼Œç„¶åå°†å…¶å†™å…¥req_token_pool
1. â€‹â€‹è§£ç ï¼ˆdecodeï¼‰â€‹â€‹ï¼š
    1. req_to_token_pool.allocï¼šä¸éœ€è¦
    2. token_to_kv_pool_allocate.allocï¼šéœ€è¦ï¼Œå› ä¸ºæ¯æ¬¡è§£ç ä¸€ä¸ªæ–°token

å› æ­¤ï¼Œåœ¨scheduler.get_next_batch_to_runä¸­è·å–ScheduleBatchæ—¶ï¼Œä¸åŒé˜¶æ®µæœ‰ä¸åŒçš„é€»è¾‘æ¥å¤„ç†åˆ†é…å’Œé‡Šæ”¾æ§½ä½ã€‚

```python
class ScheduleBatch:
    """å­˜å‚¨è°ƒåº¦å™¨ä¸Šä¸€æ‰¹æ¬¡çš„æ‰€æœ‰ä¿¡æ¯"""

  def prepare_for_extend():
    bs = len(reqs)
    req_pool_indices = alloc_req_slots(bs)

    # fill_ids = origin_input_ids + output_ids
    # input_idsæ˜¯éœ€è¦è®¡ç®—KVç¼“å­˜çš„token_ids
    input_ids = [r.fill_ids[len(r.prefix_indices): ] for r in reqs]

    # è¿™æ˜¯éœ€è¦åˆ†é…æ§½ä½ä»¥å®¹çº³çš„tokenæ•°é‡
    extend_num_tokens = sum(len(ids) for ids in input_ids)

    seq_lens = [len(r.fill_ids) for r in reqs]
    prefix_lens = [len(r.prefix_indices) for r in reqs]

    # extend_lenså®é™…ä¸Šç­‰äº`seq_lens - prefix_lens`
    extend_lens = [r.extend_input_len for r in reqs]

    for i, (req, seq_len, pre_len) in enumerate(reqs, seq_lens, pre_lens):
      req.req_pool_idx = req_pool_indices[i]

      # å†æ¬¡ç¡®è®¤
      assert seq_len - pre_len == req.extend_input_len

      if pre_len > 0:
        # å°†ç¼“å­˜çš„`out_cache_loc`å†™å…¥`req_to_token_pool`
        req_to_token_pool.write(
                    (req.req_pool_idx, slice(0, pre_len)), req.prefix_indices
                )

       out_cache_loc = alloc_token_slots(extend_num_tokens)

       pt = 0
       for i in range(bs):
         # å°†æœªç¼“å­˜çš„`out_cache_loc`å†™å…¥`req_to_token_pool`
            for i in range(bs):
                self.req_to_token_pool.write(
                    (req_pool_indices[i], slice(prefix_lens[i], seq_lens[i])),
                    out_cache_loc[pt : pt + extend_lens[i]],
                )
                pt += extend_lens[i]
       ... END !!!

  def prepare_for_decode():
    bs = len(reqs)

    # åˆ†é…`bs`ä¸ªtoken
    out_cache_loc = self.alloc_token_slots(bs)

    # è®¡ç®—`req_to_token_pool`ä½ç½®
    locs = seq_lens + 1

    # å†™å…¥
    req_to_token_pool.write(
            (req_pool_indices, locs), out_cache_loc.to(torch.int32)
        )
       ... END !!!

  def alloc_req_slots(num_reqs):
    req_pool_indices = req_to_token_pool.alloc(num_reqs)
    if req_pool_indices is None: raise RuntimeError("")
    return req_pool_indices

  def alloc_token_slots(num_tokens):
    out_cache_loc = self.token_to_kv_pool_allocator.alloc(num_tokens)
    if out_cache_loc is None: raise RuntimeError()
    return out_cache_loc
```

**è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶è¯»å–å’Œä¿å­˜å®é™…KVç¼“å­˜æ•°æ®**
åœ¨å‰å‘ä¼ æ’­ä¸­ï¼Œmodel_runnerä¼šè°ƒç”¨attention_backnend.init_forward_metadataåˆå§‹åŒ–æ³¨æ„åŠ›åç«¯çš„å…ƒæ•°æ®ï¼Œç„¶åè°ƒç”¨å®é™…çš„forward_extendå’Œforward_decode

åœ¨init_forward_metadataä¸­ï¼Œé€šè¿‡req_to_token_pool.req_to_tokenè·å–é¡µè¡¨ï¼Œç”¨äºæ¯å±‚æ³¨æ„åŠ›åˆ†æ•°çš„è®¡ç®—

```python
class FlashAttentionBackend(AttentionBackend):
  def init_forward_metadata(forward_batch):
    metadata = FlashAttentionMetadata()
    if forward_batch.is_decode():
      metadata.max_seq_len_k = forward_batch.seq_lens_cpu.max().item()
      # è·å–é¡µè¡¨ï¼
      metadata.page_table = forward_batch.req_to_token_pool.req_to_token[
                 forward_batch.req_pool_indices, : metadata.max_seq_len_k
             ]
     elif forward_batch.is_extend():
       # ... å‡ ä¹ç›¸åŒ ...
```

ä¿å­˜å’Œæ£€ç´¢è¿‡ç¨‹å‘ç”Ÿåœ¨æ¨¡å‹å‰å‘ä¼ æ’­ä¸­ï¼Œå³attention_backend.forward_extendæˆ–attention_backend.forward_extend

```python
class FlashAttention(AttentionBackend):
  def forward_extend(q, k, v, layer, forward_batch, save_kv_cache=True, ...):
    if k is not None:
      if v is not None:
        cache_loc = forward_batch.out_cache_loc

        # !!! å°†KVç¼“å­˜ä¿å­˜åˆ°token_to_kv_pool !!!
        forward_batch.token_to_kv_pool.set_kv_buffer(
                        layer, cache_loc, k, v, ...
                    )
       # ä½¿ç”¨æ‰€æœ‰å±‚é¢„è®¡ç®—çš„å…ƒæ•°æ®
        # ä¸ºFlashAttentionæ“ä½œå‡†å¤‡å…ƒæ•°æ®
        metadata = self.forward_metadata
        page_table = metadata.page_table
        cu_seqlens_q = metadata.cu_seqlens_q
        cache_seqlens = metadata.cache_seqlens_int32
        max_seqlen_q = metadata.max_seq_len_q
        max_seqlen_k = metadata.max_seq_len_k
        cu_seqlens_k = metadata.cu_seqlens_k

        # !!! ä»token_to_kv_poolæ£€ç´¢KVç¼“å­˜ !!!
        key_cache, value_cache = forward_batch.token_to_kv_pool.get_kv_buffer(
                layer.layer_id
            )
        # æ£€æŸ¥æ ¼å¼
        key_cache = key_cache.view(
                -1, self.page_size, layer.tp_k_head_num, layer.head_dim
            )
        value_cache = value_cache.view(
                -1, self.page_size, layer.tp_v_head_num, layer.head_dim
            )

        result = flash_attn_with_kvcache(
          q=q.contiguous().view(-1, layer.tp_q_head_num, layer.head_dim),
          key_cache,
          value_cache,
          page_table,
          ...
       )

       return o.view(-1, layer.tp_q_head_num * layer.v_head_dim)

  def forward_decode(forward_batch):
    # ... å‡ ä¹ä¸forward_extendç›¸åŒ ...
```

ç¬¬ä¸€éƒ¨åˆ†KVç¼“å­˜ç®¡ç†åˆ°æ­¤ç»“æŸï¼Œæˆ‘ä»¬è®¨è®ºäº†ï¼š

KVç¼“å­˜å¦‚ä½•åˆå§‹åŒ–
KVç¼“å­˜å¦‚ä½•ç®¡ç†ï¼ˆä¸ºè¯·æ±‚åˆ†é…æ§½ä½å’Œtokenï¼‰
è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°æ—¶å¦‚ä½•ä¿å­˜å’Œæ£€ç´¢å®é™…KVç¼“å­˜æ•°æ®

## Radix Tree Cache

SGLangçš„ä¸€ä¸ªåˆ›æ–°æ€æƒ³æ˜¯åŸºæ•°æ³¨æ„åŠ›ï¼Œå®ƒä½¿ç”¨åŸºæ•°æ ‘å°½å¯èƒ½å¤ç”¨KVç¼“å­˜

é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯åŸºæ•°æ ‘ï¼Ÿ

å…¶æ ¸å¿ƒæ€æƒ³æ˜¯è·å–å‰ç¼€

### Radix Tree

```python
class TreeNode:
    counter = 0

    def __init__(self, id: Optional[int] = None):
        self.children = defaultdict(TreeNode)  # ä½¿ç”¨1é¡µå¤§å°çš„keyä½œä¸ºå­—å…¸é”®
        self.parent = None
        self.key = None  # Keyæ˜¯`token_ids`
        self.value = None  # Valueæ˜¯`out_cache_loc`ï¼Œè®°å½•å®é™…KVç¼“å­˜æ•°æ®çš„ä½ç½®

        self.lock_ref = 0  # æœ‰å¤šå°‘è¯·æ±‚å¼•ç”¨æ­¤èŠ‚ç‚¹

        self.last_access_time = time.monotonic()

        self.hit_count = 0

        # è¡¨ç¤ºèŠ‚ç‚¹æ­£åœ¨ä»ä¸»æœºåŠ è½½KVç¼“å­˜
        self.loading = False

        # å­˜å‚¨KVç¼“å­˜çš„ä¸»æœºç´¢å¼•
        self.host_value = None

        self.id = TreeNode.counter if id is None else id
        TreeNode.counter += 1

class RadixTree(BasePrefixCache):
  def __init__(req_to_token_pool, token_to_kv_pool_allocator, page_size, ...):
    if page_size == 1:
      # key_match_fnï¼šç»™å®šä¸¤ä¸ªkeyï¼Œè¿”å›å®ƒä»¬å…±æœ‰çš„å‰ç¼€idsæ•°é‡
            key_match_fn = _key_match_page_size1

            # get_child_key_fnï¼šè·å–1é¡µå¤§å°çš„key
            get_child_key_fn = lambda key: key[0]
        else:
            key_match_fn = partial(_key_match_paged, page_size=page_size)
            get_child_key_fn = lambda key: tuple(key[:page_size])
    reset()

  def reset(self):
        self.root_node = TreeNode()
        self.root_node.key = []
        self.root_node.value = []
        self.root_node.lock_ref = 1
        self.evictable_size_ = 0
        self.protected_size_ = 0
        self._record_all_cleared_event()
```

#### åŒ¹é…

```python
  ########################
   # åŒ¹é…å‰ç¼€
   ########################
   def match_prefix(key: List[int]):
     page_aligned_len = len(key) // page_size * page_size
       key = key[:page_aligned_len]

       value, last_node = _match_prefix_helper(root_node, key)
       if value: value = torch.cat(value)
       else: value = torch.empty((0,), dtype=torch.int64, device=device)

       # 1. åŸºæ•°æ ‘ä¸­çš„å‰ç¼€`out_cache_loc`
       # 2. last_node
      return value, last_node

  def _match_prefix_helper(node, key):
    # æ›´æ–°æ—¶é—´
    node.last_access_time = time.monotonic()

    # å…ˆè·å–å­key
    child_key = self.get_child_key_fn(key)

    value = []
    while len(key) > 0 and child_key in node.children.keys():

      child = node.children[child_key]

      # æ›´æ–°æ—¶é—´
      child.last_access_time = time.monotonic()

      # è·å–å‰ç¼€idsçš„æ•°é‡ï¼ˆn * page_sizeï¼‰
      prefix_len = self.key_match_fn(child.key, key)

      if prefix_len < len(child.key):
        # ä¸å®Œå…¨åŒ¹é…ï¼Œæ‹†åˆ†ä¸€ä¸ªå®Œå…¨åŒ¹é…ä½†æ›´çŸ­çš„new_node

        # æ³¨æ„ï¼šprefix_lenè‡³å°‘ä¸º1é¡µå¤§å°ï¼Œå› ä¸º`child_key in node.children.keys()`
        new_node = self._split_node(child.key, child, prefix_len)

        # è¿½åŠ åŒ¹é…çš„å€¼
        value.append(new_node.value)
               node = new_node
               break
      else:
        # å®Œå…¨åŒ¹é…ï¼Œå°è¯•è·å–ä¸‹ä¸€ä¸ªå­èŠ‚ç‚¹

        # ä¿å­˜å€¼
        value.append(child.value)

        # æ›´æ–°èŠ‚ç‚¹
               node = child

               # æˆªæ–­å·²åŒ¹é…çš„å‰ç¼€key
               key = key[prefix_len:]

               if len(key):
                 child_key = self.get_child_key_fn(key)
       return value, node
```

æ‹†åˆ†èŠ‚ç‚¹ï¼š

```
  #############
   # æ‹†åˆ†èŠ‚ç‚¹
   #############
  def _split_node(key: List[int], child, split_len):
    # è¿™é‡Œçš„keyå®é™…ä¸Šæ˜¯å­èŠ‚ç‚¹çš„key
    # keyå’Œvalueå°†è¢«åˆ†æˆä¸¤éƒ¨åˆ†
    # keyå’Œvalue: [......................... | ..........................]
    #                                       prefix_len
    #                  å·¦ä¾§ï¼šæ–°èŠ‚ç‚¹çš„kv        å³ä¾§ï¼šæˆªæ–­çš„å­èŠ‚ç‚¹
    # æ‹†åˆ†åï¼Œ`child(node)`å°†å˜ä¸º
    # `parent <-> child`    =>
    # `parent <-> new_node <-> truncated child`

    # åˆ›å»ºæ–°èŠ‚ç‚¹
    new_node = TreeNode()

    # ä½¿`new_node ---æˆªæ–­å­èŠ‚ç‚¹çš„1é¡µå¤§å°key---> child`
    new_node.children = {self.get_child_key_fn(key[split_len:]): child}

       # ä½¿`parent -> new_node`
       new_node.parent = child.parent

       # ä½¿new_nodeè·å¾—ç›¸åŒçš„å¼•ç”¨è®¡æ•°
       new_node.lock_ref = child.lock_ref

       # è·å–å·¦ä¾§kvï¼Œå¹¶è®¾ç½®ç»™new_node
       new_node.key = child.key[:split_len]
       new_node.value = child.value[:split_len]

    # ä½¿`new_node <- child`
       child.parent = new_node

       # ä½¿`child`å˜ä¸º`æˆªæ–­çš„å­èŠ‚ç‚¹`ï¼šæˆªæ–­split_lençš„keyå’Œvalue
       child.key = child.key[split_len:]
       child.value = child.value[split_len:]

       # ä½¿`parent ----new_nodeçš„1é¡µå¤§å°key---> new_node
       new_node.parent.children[self.get_child_key_fn(key)] = new_node

    return new_node
```

#### æ’å…¥èŠ‚ç‚¹

```python
 ################
 # æ’å…¥èŠ‚ç‚¹
 ################
 def insert(self, key: List, value=None):
     if self.disable: return 0

     if value is None: value = [x for x in key]

     return _insert_helper(root_node, key, value)

  def _insert_helper(node, key, value):
    # æ›´æ–°èŠ‚ç‚¹æ—¶é—´ç”¨äºLRUæ·˜æ±°
    node.last_access_time = time.monotonic()

      if len(key) == 0: return 0

      # è·å–ç”¨äºæœç´¢å‰ç¼€çš„1é¡µå¤§å°key
      child_key = get_child_key_fn(key)

      total_prefix_length = 0

      while len(key) > 0 and child_key in node.children.keys():
      # è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
      node = node.children[child_key]
      # æ›´æ–°ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„æ—¶é—´
      node.last_access_time = time.monotonic()

      # è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹å’ŒæŸ¥è¯¢keyçš„å‰ç¼€é•¿åº¦
      prefix_len = self.key_match_fn(node.key, key)

      total_prefix_length += prefix_len

      # æ›´æ–°keyå’Œvalue
      key = key[prefix_len:]
          value = value[prefix_len:]

          if prefix_len < len(node.key):
            # ä¸å®Œå…¨åŒ¹é…ï¼Œæ‹†åˆ†èŠ‚ç‚¹
            new_node = _split_node(node.key, node, prefix_len)

              node = new_node

          if len(key):
            # ä»æœ‰éƒ¨åˆ†keyæœªåŒ¹é…ï¼Œå°è¯•ç»§ç»­æŸ¥æ‰¾ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
            child_key = get_child_key_fn(key)

            # æ³¨æ„ï¼šå¦‚æœprefix_len < len(node.key)
            # åˆ™æ— æ³•ç»§ç»­æ­¤whileå¾ªç¯
            # å› ä¸ºæ‹†åˆ†åçš„æ–°èŠ‚ç‚¹åªæœ‰ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œå³æœªåŒ¹é…çš„èŠ‚ç‚¹
            # æ‰€ä»¥è¿™ä¸ªæ–°çš„`child_key`ä¸åœ¨`node.children.keys()`ä¸­
            # æ­¤whileå¾ªç¯ä»…åœ¨å®Œå…¨åŒ¹é…ä½†æŸ¥è¯¢keyä»æœ‰å‰©ä½™éƒ¨åˆ†æ—¶ç»§ç»­

   if len(key):
     # å¦‚æœä»æœ‰æœªåŒ¹é…çš„å‰©ä½™keyï¼Œ
     # åˆ›å»ºæ–°èŠ‚ç‚¹
     # æ³¨æ„ï¼šæ­¤æ–°èŠ‚ç‚¹çš„lock_refä¸º0ï¼Œå› æ­¤å¯è¢«æ·˜æ±°
     new_node = TreeNode()
          new_node.parent = node
          new_node.key = key
          new_node.value = value

          # ä½¿node`æŒ‡å‘æ­¤`new_node`
          node.children[child_key] = new_node

          # è¿™æ˜¯å¯æ·˜æ±°çš„ï¼Œå› ä¸ºå®ƒæ˜¯å¶èŠ‚ç‚¹
          evictable_size_ += len(value)

   return total_prefix_length
```

#### API

- è¯·æ±‚å®Œæˆæˆ–æœªå®Œæˆæ—¶çš„ç¼“å­˜
- åˆ é™¤ä¸éœ€è¦çš„ç¼“å­˜

```python
 #######################
 # ç¼“å­˜æœªå®Œæˆçš„è¯·æ±‚
  #######################
  def cache_unfinished_req(req):
    token_ids = req.fill_ids

    # è·å–`out_cache_loc`ï¼Œå³Value
    kv_indices = req_to_token_pool.req_to_token[
            req.req_pool_idx, : len(token_ids)
      ]

      if page_size != 1:
        page_aligned_len = len(kv_indices) // page_size * page_size
        # å¯¹é½V
          page_aligned_kv_indices = kv_indices[:page_aligned_len].clone()
      else:
          page_aligned_len = len(kv_indices)
          page_aligned_kv_indices = kv_indices.clone()

      # å¯¹é½K
      page_aligned_token_ids = token_ids[:page_aligned_len]

      # æ’å…¥K,V
      new_prefix_len = insert(page_aligned_token_ids, page_aligned_kv_indices)

      # ç§»é™¤é‡å¤éƒ¨åˆ†
      token_to_kv_pool_allocator.free(
            kv_indices[len(req.prefix_indices) : new_prefix_len]
      )

      # è·å–å‰ç¼€`out_cache_loc`å’Œ`new_last_node`
      new_indices, new_last_node = self.match_prefix(page_aligned_token_ids)

      # ä»…å†™å…¥æ–°çš„`out_cache_loc`
      req_to_token_pool.write(
            (req.req_pool_idx, slice(len(req.prefix_indices), len(new_indices))),
            new_indices[len(req.prefix_indices) :],
      )

      # root -> ... -> last_node -> ... -> new_last_node
      # |-- lock_ref - 1 --|
      dec_lock_ref(req.last_node)

      # root -> ... -> last_node -> ... -> new_last_node
      # |------------- lock_ref + 1 -----------------|
      inc_lock_ref(new_last_node)


 #####################
 # ç¼“å­˜å®Œæˆçš„è¯·æ±‚
 #####################
  def cache_finished_req(req):
   if self.disable:
     # å¦‚æœç¦ç”¨åŸºæ•°æ ‘ï¼Œç›´æ¥é‡Šæ”¾æ­¤å®Œæˆè¯·æ±‚çš„KVç¼“å­˜

     # è·å–`out_cache_loc`
     kv_indices = req_to_token_pool.req_to_token[
              req.req_pool_idx, : len(req.origin_input_ids) + len(req.output_ids) - 1
          ]

          # é‡Šæ”¾`reqæ§½ä½`å’Œ`token_to_kv_poolæ§½ä½`
          token_to_kv_pool_allocator.free(kv_indices)
          req_to_token_pool.free(req.req_pool_idx)
          return

     # å¦‚æœä½¿ç”¨åŸºæ•°æ ‘ï¼Œä¸ç«‹å³é‡Šæ”¾KVç¼“å­˜ä»¥ä¾¿å¤ç”¨

     # è·å–token_idsï¼Œå³key
     token_ids = (req.origin_input_ids + req.output_ids)[:-1]

     # è·å–`out_cache_loc`ï¼Œå³value
     kv_indices = req_to_token_pool.req_to_token[
        req.req_pool_idx, : len(token_ids)
    ]

    # å‡è®¾é¡µå¤§å°ä¸º1ï¼Œå› æ­¤è‡ªåŠ¨å¯¹é½
    page_aligned_len = len(kv_indices)
     page_aligned_kv_indices = kv_indices.clone()

    # å°†[token_ids, out_cache_loc]æ’å…¥åŸºæ•°æ ‘ä»¥ä¾¿å¤ç”¨
    new_prefix_len = insert(
         token_ids[:page_aligned_len], page_aligned_kv_indices
    )

     # ä»…é‡Šæ”¾[len(prefix_indices): new_prefix_len]éƒ¨åˆ†çš„kvæ± ï¼Œä¸ºä»€ä¹ˆï¼Ÿ
     # å› ä¸ºè¿™éƒ¨åˆ†`out_cache_loc`æ˜¯é‡å¤çš„ï¼ˆå†—ä½™çš„ï¼‰ï¼

     # æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹ï¼š
     # `req.prefix_indices`åœ¨é¦–æ¬¡è°ƒåº¦æ—¶è®¡ç®—
     # `new_prefix_len`æ˜¯å®Œæˆæ—¶çš„å‰ç¼€é•¿åº¦
     # [len(req.prefix_indices): new_prefix_len]æ˜¯è®¡ç®—è¿‡ç¨‹ä¸­é‡å¤çš„éƒ¨åˆ†
    token_to_kv_pool_allocator.free(
          kv_indices[len(req.prefix_indices) : new_prefix_len]
     )

     # é‡Šæ”¾`reqæ§½ä½`
     # å› ä¸ºè¯·æ±‚å·²å®Œæˆï¼Œå…¶req_pool_idxå¯ç”¨äºå…¶ä»–è¯·æ±‚
     req_to_token_pool.free(req.req_pool_idx)

     # å‡å°‘æ‹¥æœ‰out_cache_loc[:len(prefix_indices)]çš„èŠ‚ç‚¹çš„lock_ref
     # è¿™äº›éƒ¨åˆ†å¯èƒ½å˜ä¸ºå¯æ·˜æ±°
     # ä½†æ³¨æ„ï¼šè¿™äº›`out_cache_loc`å°šæœªè¢«æ·˜æ±°
     dec_lock_ref(req.last_node)
```

```python
  def evict(num_tokens: int):
    if disable: return

    leaves = _collect_leaves()

    # æŒ‰`last_access_time`æ’åºï¼ˆLRUï¼‰
    heapq.heapify(leaves)

    num_evicted = 0
    while num_evicted < num_tokens and len(leaves):
      x = heapq.heappop(leaves)
      if x == self.root_node: break

      # å¦‚æœæœ‰è¯·æ±‚æŒ‡å‘æ­¤èŠ‚ç‚¹ï¼Œè·³è¿‡
            if x.lock_ref > 0: continue

            # é‡Šæ”¾æ­¤èŠ‚ç‚¹çš„`out_cache_loc`
            token_to_kv_pool_allocator.free(x.value)

            num_evicted += len(x.value)
            _delete_leaf(x)

            # ä¸ºä¸‹ä¸€æ¬¡æ·˜æ±°æ·»åŠ æ–°çš„å¶èŠ‚ç‚¹
            if len(x.parent.children) == 0:
                heapq.heappush(leaves, x.parent)

  def _delete_leaf(node):

    # ä»çˆ¶èŠ‚ç‚¹ä¸­åˆ é™¤æ­¤èŠ‚ç‚¹
    for k, v in node.parent.children.items():
            if v == node:
                break
        del node.parent.children[k]

        # æ›´æ–°å¯æ·˜æ±°å¤§å°
        evictable_size_ -= len(node.key)

```

-- --
**ä½¿ç”¨æ–¹å¼**

1. å½“prefillç»“æŸæ—¶ï¼Œ

```python
def process_batch_result_prefill(batch, result):
  for i, (req, next_token_id) in enumerate(batch.reqs, result.next_token_ids):
    req.output_ids.append(next_token_id)
        req.check_finished()

        if req.finished():
          tree_cache.cache_finished_req(req)

       elif not batch.decoding_reqs or req not in batch.decoding_reqs:
            # æ›´æ–°åŸºæ•°æ ‘ä»¥ä¾¿å…¶ä»–è¯·æ±‚åŒ¹é…
            tree_cache.cache_unfinished_req(req)
```

2. å½“decodeç»“æŸæ—¶ï¼Œ

```python
def process_batch_result_decode(batch, result):
  for i, (req, next_token_id) in enumerate(zip(batch.reqs, next_token_ids)):
    req.check_finished()

    if req.finished():
           tree_cache.cache_finished_req(req)
```

<aside> ğŸ’¡
åªæœ‰åœ¨decodeå®Œæˆæ—¶ï¼Œtree_cacheæ‰ä¼šç¼“å­˜å…¶ï¼ˆtoken_ids, out_cache_locï¼‰

</aside>

**åˆ é™¤ä¸éœ€è¦çš„ç¼“å­˜**:
å½“token_to_kv_poolä¸­çš„available_sizeæ— æ³•æ”¯æŒä¼ å…¥è¯·æ±‚æ—¶ï¼Œä¼šå‘ç”Ÿæ·˜æ±°ï¼ˆå³é‡Šæ”¾out_cache_locï¼‰

```python
def alloc_token_slots(num_tokens: int, backup_state: bool = False):
    if token_to_kv_pool_allocator.available_size() < num_tokens:
      if tree_cache is not None:
          tree_cache.evict(num_tokens)

  out_cache_loc = token_to_kv_pool_allocator.alloc(num_tokens)
```

## å‚è€ƒ

- [https://hebiao064.github.io/fa3-attn-backend-basic](https://hebiao064.github.io/fa3-attn-backend-basic)
